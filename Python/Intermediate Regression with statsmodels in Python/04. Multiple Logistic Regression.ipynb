{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with two explanatory variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression also supports multiple explanatory variables. To include multiple explanatory variables in logistic regression models, the syntax is the same as for linear regressions.\n",
    "\n",
    "Here you'll fit a model of churn status with both of the explanatory variables from the dataset: the length of customer relationship and the recency of purchase, and their interaction.\n",
    "\n",
    "`churn` is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import logit\n",
    "# from statsmodels.formula.api import logit\n",
    "\n",
    "# # Fit a logistic regression of churn status vs. length of relationship, recency, and an interaction\n",
    "# mdl_churn_vs_both_inter = logit(\"has_churned ~ time_since_first_purchase + time_since_last_purchase + time_since_first_purchase : time_since_last_purchase\", data = churn).fit()\n",
    "\n",
    "# # Print the coefficients\n",
    "# print(mdl_churn_vs_both_inter.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with linear regression, the joy of logistic regression is that you can make predictions. Let's step through the prediction flow one more time!\n",
    "\n",
    "`churn` and `mdl_churn_vs_both_inter` are available; `itertools.product` is loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create time_since_first_purchase\n",
    "# time_since_first_purchase = np.arange(-2, 4.1, 0.1)\n",
    "\n",
    "# # Create time_since_last_purchase\n",
    "# time_since_last_purchase = np.arange(-1, 6.1, 0.1)\n",
    "\n",
    "# # Create p as all combinations of values of time_since_first_purchase and time_since_last_purchase\n",
    "# p = product(time_since_first_purchase, time_since_last_purchase)\n",
    "\n",
    "# # Transform p to a DataFrame and name the columns\n",
    "# explanatory_data = pd.DataFrame(p, columns=[\"time_since_first_purchase\",\n",
    "#                                             \"time_since_last_purchase\"])\n",
    "\n",
    "# # Create prediction_data\n",
    "# prediction_data = explanatory_data.assign(\n",
    "#     has_churned = mdl_churn_vs_both_inter.predict(explanatory_data)\n",
    "# )\n",
    "\n",
    "# # Create most_likely_outcome\n",
    "# prediction_data[\"most_likely_outcome\"] = np.round(prediction_data[\"has_churned\"])\n",
    "\n",
    "# # See the result\n",
    "# print(prediction_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing multiple explanatory variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting has similar issues as with the linear regression case: it quickly becomes difficult to include more numeric variables in the plot. Here you'll look at the case of two numeric explanatory variables, and the solution is basically the same as before: use color to denote the response.\n",
    "\n",
    "Here there are only two possible values of response (zero and one), both in the actual dataset and the predicted dataset.\n",
    "\n",
    "`churn` and `prediction_data` are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Using churn, plot recency vs. length of relationship, colored by churn status\n",
    "# sns.scatterplot(x=\"time_since_first_purchase\",\n",
    "#                 y=\"time_since_last_purchase\",\n",
    "#                 data=churn, \n",
    "#                 hue=\"has_churned\")\n",
    "\n",
    "# # Using prediction_data, plot recency vs. length of relationship, colored by most_likely_outcome\n",
    "# sns.scatterplot(x=\"time_since_first_purchase\",\n",
    "#                 y=\"time_since_last_purchase\",\n",
    "#                 data=prediction_data, \n",
    "#                 hue=\"most_likely_outcome\",\n",
    "#      alpha=0.2,\n",
    "#      legend=False)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the response variable has just two outcomes, like the case of churn, the measures of success for the model are \"how many cases where the customer churned did the model correctly predict?\" and \"how many cases where the customer didn't churn did the model correctly predict?\". These can be found by generating a confusion matrix and calculating summary metrics on it.\n",
    "\n",
    "Recall the following definitions:\n",
    "\n",
    "- Accuracy is the proportion of predictions that are correct.\n",
    "- Sensitivity is the proportion of true observations that are correctly predicted by the model as being true.\n",
    "- Specificity is the proportion of false observations that are correctly predicted by the model as being false.\n",
    " \n",
    "\n",
    "`churn` and `mdl_churn_vs_both_inter` are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create conf_matrix\n",
    "# conf_matrix = mdl_churn_vs_both_inter.pred_table()\n",
    "\n",
    "# # Extract TN, TP, FN and FP from conf_matrix\n",
    "# TN = conf_matrix[0,0]\n",
    "# TP = conf_matrix[1,1]\n",
    "# FN = conf_matrix[1,0]\n",
    "# FP = conf_matrix[0,1]\n",
    "\n",
    "# # Calculate and print the accuracy\n",
    "# accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "# print(\"accuracy\", accuracy)\n",
    "\n",
    "# # Calculate and print the sensitivity\n",
    "# sensitivity = TP / (TP + FN)\n",
    "# print(\"sensitivity\", sensitivity)\n",
    "\n",
    "# # Calculate and print the specificity\n",
    "# specificity = TN / (TN + FP)\n",
    "# print(\"specificity\", specificity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative distribution function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the logistic distribution is key to understanding logistic regression. Like the normal (Gaussian) distribution, it is a probability distribution of a single continuous variable. Here you'll visualize the cumulative distribution function (CDF) for the logistic distribution. That is, if you have a logistically distributed variable, x, and a possible value, xval, that x could take, then the CDF gives the probability that x is less than xval.\n",
    "\n",
    "The logistic distribution's CDF is calculated with the logistic function (hence the name). The plot of this has an S-shape, known as a sigmoid curve. An important property of this function is that it takes an input that can be any number from minus infinity to infinity, and returns a value between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import logistic\n",
    "# from scipy.stats import logistic\n",
    "\n",
    "# # Create x ranging from minus ten to ten in steps of 0.1\n",
    "# x = np.arange(-10, 10.1, 0.1)\n",
    "\n",
    "# # Create logistic_dist\n",
    "# logistic_dist = pd.DataFrame({\"x\": x,\n",
    "#                               \"log_cdf\": logistic.cdf(x),\n",
    "#                               \"log_cdf_man\": 1 / (1 + np.exp(-x))})\n",
    "\n",
    "# # Using logistic_dist, plot log_cdf vs. x\n",
    "# sns.lineplot(x = \"x\", y= \"log_cdf\", data = logistic_dist)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse cumulative distribution function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function (logistic distribution CDF) has another important property: each x input value is transformed to a unique value. That means that the transformation can be reversed. The logit function is the name for the inverse logistic function, which is also the logistic distribution inverse cumulative distribution function. (All three terms mean exactly the same thing.)\n",
    "\n",
    "The logit function takes values between zero and one, and returns values between minus infinity and infinity.\n",
    "\n",
    "`logistic` is available from `scipy.stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create p ranging from 0.001 to 0.999 in steps of 0.001\n",
    "# p = np.arange(0.001, 1, 0.001)\n",
    "\n",
    "# # Create logistic_dist_inv\n",
    "# logistic_dist_inv = pd.DataFrame({\"p\": p,\n",
    "#                                   \"logit\": logistic.ppf(p),\n",
    "#                                   \"logit_man\": np.log(p / (1 - p))})\n",
    "\n",
    "# # Using logistic_dist_inv, plot logit vs. p\n",
    "# sns.lineplot(x = \"p\", y = \"logit\", data = logistic_dist_inv)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic distribution parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic CDF is not just a single curve. In the same way that the normal distribution has mean and standard deviation parameters that affect the CDF curve, the logistic distribution has location and scale parameters. Here, you'll visualize how changing those parameters changes the CDF curve.\n",
    "\n",
    "How do changes to the parameters change the CDF curve?\n",
    "\n",
    "<center><img src=\"images/04.091.jpg\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/04.092.jpg\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/04.093.jpg\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As `location` increases, the logistic CDF curve moves rightwards. As `scale` increases, the steepness of the slope decreases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likelihood & log-likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression tries to optimize a \"sum of squares\" metric in order to find the best fit. That metric isn't applicable to logistic regression. Instead, logistic regression tries to optimize a metric called likelihood, or a related metric called log-likelihood.\n",
    "\n",
    "The dashboard shows churn status versus time since last purchase from the `churn` dataset. The blue dotted line is the logistic regression prediction line. (That is, it's the \"best fit\" line.) The black solid line shows a prediction line calculated from the intercept and slope coefficients you specify as `logistic.cdf(intercept + slope * time_since_last_purchase)`.\n",
    "\n",
    "Change the intercept and slope coefficients and watch how the likelihood and log-likelihood values change.\n",
    "\n",
    "As you get closer to the best fit line, what statement is true about likelihood and log-likelihood?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/04.111.jpg\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/04.112.jpg\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/04.113.jpg\"  style=\"width: 400px, height: 300px;\"/></center>\n",
    "<center><img src=\"images/04.114.jpg\"  style=\"width: 400px, height: 300px;\"/></center>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both likelihood and log-likelihood increase to a maximum value. Logistic regression chooses the prediction line that gives you the maximum likelihood value. It also gives maximum log-likelihood."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into the internals and implement a logistic regression algorithm. Since `statsmodels`'s `logit()` function is very complex, you'll stick to implementing simple logistic regression for a single dataset.\n",
    "\n",
    "Rather than using sum of squares as the metric, we want to use likelihood. However, log-likelihood is more computationally stable, so we'll use that instead. Actually, there is one more change: since we want to maximize log-likelihood, but `minimize()` defaults to finding minimum values, it is easier to calculate the negative log-likelihood.\n",
    "\n",
    "The log-likelihood value for each observation is\n",
    "\n",
    "The metric to calculate is the negative sum of these log-likelihood contributions.\n",
    "\n",
    "The explanatory values (the time_since_last_purchase column of churn) are available as x_actual. The response values (the has_churned column of churn) are available as y_actual. logistic is imported from scipy.stats, and `logit()` and `minimize()` are also loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Complete the function\n",
    "# def calc_neg_log_likelihood(coeffs):\n",
    "#     # Unpack coeffs\n",
    "#     intercept, slope = coeffs\n",
    "#     # Calculate predicted y-values\n",
    "#     y_pred = logistic.cdf(intercept + slope * x_actual)\n",
    "#     # Calculate log-likelihood\n",
    "#     log_likelihood = np.log(y_pred) * y_actual + np.log(1 - y_pred) * (1 - y_actual)\n",
    "#     # Calculate negative sum of log_likelihood\n",
    "#     neg_sum_ll = -np.sum(log_likelihood)\n",
    "#     # Return negative sum of log_likelihood\n",
    "#     return neg_sum_ll\n",
    "  \n",
    "# # Call minimize on calc_sum_of_squares  \n",
    "# print(minimize(fun=calc_neg_log_likelihood,\n",
    "#                x0=[0,0]))\n",
    "\n",
    "# # Compare the output with the logit() call.\n",
    "# print(logit(\"has_churned ~ time_since_last_purchase\", data=churn).fit().params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
