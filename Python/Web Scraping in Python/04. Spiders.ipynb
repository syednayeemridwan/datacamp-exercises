{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inheriting the Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class is roughly a collection of related variables and functions housed together. Sometimes one class likes to use methods from another class, and so we will inherit methods from a different class. That's what we do in the spider class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scrapy library\n",
    "# import scrapy\n",
    "\n",
    "# # Create the spider class\n",
    "# class YourSpider(scrapy.Spider):\n",
    "#   name = \"your_spider\"\n",
    "#   # start_requests method\n",
    "#   def start_requests(self):\n",
    "#     pass\n",
    "#   # parse method\n",
    "#   def parse(self, response):\n",
    "#     pass\n",
    "  \n",
    "# # Inspect Your Class\n",
    "# inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurl the URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've written a function inspect_class which will print out the list of elements you have in the urls variable within the start_requests method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scrapy library\n",
    "# import scrapy\n",
    "\n",
    "# # Create the spider class\n",
    "# class YourSpider( scrapy.Spider ):\n",
    "#   name = \"your_spider\"\n",
    "#   # start_requests method\n",
    "#   def start_requests( self ):\n",
    "#     urls = [\"https://www.datacamp.com\" ,\"https://scrapy.org\"]\n",
    "#     for url in urls:\n",
    "#       yield url\n",
    "#   # parse method\n",
    "#   def parse( self, response ):\n",
    "#     pass\n",
    "  \n",
    "# # Inspect Your Class\n",
    "# inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Referencing is Classy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " within the spider class, we always input the argument self in the start_requests and parse methods (just look in the sample code in this exercise!). This allows us to reference between methods within the class. That is, if we want to refer to the method parse within the start_requests method, we would need to write self.parse rather than just parse; what writing self does is tell the code: \"Look in the same class as start_requests for a method called parse to use.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scrapy library\n",
    "# import scrapy\n",
    "\n",
    "# # Create the spider class\n",
    "# class YourSpider( scrapy.Spider ):\n",
    "#   name = \"your_spider\"\n",
    "#   # start_requests method\n",
    "#   def start_requests( self ):\n",
    "#     self.print_msg( \"Hello World!\" )\n",
    "#   # parse method\n",
    "#   def parse( self, response ):\n",
    "#     pass\n",
    "#   # print_msg method\n",
    "#   def print_msg( self, msg ):\n",
    "#     print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# # Inspect Your Class\n",
    "# inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting with Start Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have another toy-model spider which doesn't actually scrape anything, but gives you a chance to play with the start_requests method so to start becomming familiar with the arguments to pass into the scrapy.Request call within start_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scrapy library\n",
    "# import scrapy\n",
    "\n",
    "# # Create the spider class\n",
    "# class YourSpider( scrapy.Spider ):\n",
    "#   name = \"your_spider\"\n",
    "#   # start_requests method\n",
    "#   def start_requests( self ):\n",
    "#     yield scrapy.Request( url = \"https://www.datacamp.com\", callback = self.parse )\n",
    "#   # parse method\n",
    "#   def parse( self, response ):\n",
    "#     pass\n",
    "  \n",
    "# # Inspect Your Class\n",
    "# inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pen Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your job will be to create the list of extracted author names in the parse method of the spider.\n",
    "\n",
    "Two things you should know:\n",
    "\n",
    "- You will be using the response object and the css method here.\n",
    "- The course author names are defined by the text within the paragraph p elements belonging to the class course-block__author-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the scrapy library\n",
    "# import scrapy\n",
    "\n",
    "# # Create the Spider class\n",
    "# class DCspider( scrapy.Spider ):\n",
    "#   name = 'dcspider'\n",
    "#   # start_requests method\n",
    "#   def start_requests( self ):\n",
    "#     yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "#   # parse method\n",
    "#   def parse( self, url ):\n",
    "#     # Create an extracted list of course author names\n",
    "#     author_names = url.css('p.course-block__author-name::text').extract()\n",
    "#     # Here we will just return the list of Authors\n",
    "#     return author_names\n",
    "  \n",
    "# # Inspect the spider\n",
    "# inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first chance to play with a spider which will crawl between sites (by first collecting links from one site, and following those links to parse new sites). This spider starts at the shortened DataCamp course directory, then extracts the links of the courses in the parse method; from there, it will follow those links to extract the course descriptions from each course page in the parse_descr method, and put these descriptions into the list course_descrs. Your job is to complete the code so that the spider runs as desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import the scrapy library\n",
    "# import scrapy\n",
    "\n",
    "# # Create the Spider class\n",
    "# class DCdescr( scrapy.Spider ):\n",
    "#   name = 'dcdescr'\n",
    "#   # start_requests method\n",
    "#   def start_requests( self ):\n",
    "#     yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "#   # First parse method\n",
    "#   def parse( self, response ):\n",
    "#     links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "#     # Follow each of the extracted links\n",
    "#     for link in links:\n",
    "#       yield response.follow( url = link, callback = self.parse_descr )\n",
    "      \n",
    "#   # Second parsing method\n",
    "#   def parse_descr( self, response ):\n",
    "#     # Extract course description\n",
    "#     course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "#     # For now, just yield the course description\n",
    "#     yield course_descr\n",
    "\n",
    "\n",
    "# # Inspect the spider\n",
    "# inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through creating an entire web-crawler to access course information from each course in the DataCamp course directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scrapy\n",
    "# import scrapy\n",
    "\n",
    "# # Import the CrawlerProcess: for running the spider\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# # Create the Spider class\n",
    "# class DC_Chapter_Spider(scrapy.Spider):\n",
    "#   name = \"dc_chapter_spider\"\n",
    "#   # start_requests method\n",
    "#   def start_requests(self):\n",
    "#     yield scrapy.Request(url = url_short,\n",
    "#                          callback = self.parse_front)\n",
    "#   # First parsing method\n",
    "#   def parse_front(self, response):\n",
    "#     course_blocks = response.css('div.course-block')\n",
    "#     course_links = course_blocks.xpath('./a/@href')\n",
    "#     links_to_follow = course_links.extract()\n",
    "#     for url in links_to_follow:\n",
    "#       yield response.follow(url = url,\n",
    "#                             callback = self.parse_pages)\n",
    "#   # Second parsing method\n",
    "#   def parse_pages(self, response):\n",
    "#     crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "#     crs_title_ext = crs_title.extract_first().strip()\n",
    "#     ch_titles = response.css('h4.chapter__title::text')\n",
    "#     ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "#     dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# # Initialize the dictionary **outside** of the Spider class\n",
    "# dc_dict = dict()\n",
    "\n",
    "# # Run the Spider\n",
    "# process = CrawlerProcess()\n",
    "# process.crawl(DC_Chapter_Spider)\n",
    "# process.start()\n",
    "\n",
    "# # Print a preview of courses\n",
    "# previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataCamp Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you are asked to create a CSS Locator string direct to the text of the course description. All you need to know is that from the course page, the course description text is within a paragraph p element which belongs to the class course__description (two underlines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scrapy\n",
    "# import scrapy\n",
    "\n",
    "# # Import the CrawlerProcess: for running the spider\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# # Create the Spider class\n",
    "# class DC_Description_Spider(scrapy.Spider):\n",
    "#   name = \"dc_chapter_spider\"\n",
    "#   # start_requests method\n",
    "#   def start_requests(self):\n",
    "#     yield scrapy.Request(url = url_short,\n",
    "#                          callback = self.parse_front)\n",
    "#   # First parsing method\n",
    "#   def parse_front(self, response):\n",
    "#     course_blocks = response.css('div.course-block')\n",
    "#     course_links = course_blocks.xpath('./a/@href')\n",
    "#     links_to_follow = course_links.extract()\n",
    "#     for url in links_to_follow:\n",
    "#       yield response.follow(url = url,\n",
    "#                             callback = self.parse_pages)\n",
    "#   # Second parsing method\n",
    "#   def parse_pages(self, response):\n",
    "#     # Create a SelectorList of the course titles text\n",
    "#     crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "#     # Extract the text and strip it clean\n",
    "#     crs_title_ext = crs_title.extract_first().strip()\n",
    "#     # Create a SelectorList of course descriptions text\n",
    "#     crs_descr = response.css( 'p.course__description::text' )\n",
    "#     # Extract the text and strip it clean\n",
    "#     crs_descr_ext = crs_descr.extract_first().strip()\n",
    "#     # Fill in the dictionary\n",
    "#     dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# # Initialize the dictionary **outside** of the Spider class\n",
    "# dc_dict = dict()\n",
    "\n",
    "# # Run the Spider\n",
    "# process = CrawlerProcess()\n",
    "# process.crawl(DC_Description_Spider)\n",
    "# process.start()\n",
    "\n",
    "# # Print a preview of courses\n",
    "# previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you will write the parse function for a spider and then fill in a few blanks to finish off the spider.Everything you need to know is:\n",
    "\n",
    "- The course titles are defined by the text within an h4 element whose class contains the string block__title (double underline).\n",
    "- The short course descriptions are defined by the text within a paragraph p element whose class contains the string block__description (double underline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse method\n",
    "# def parse(self, response):\n",
    "#   # Extracted course titles\n",
    "#   crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "#   # Extracted course descriptions\n",
    "#   crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "#   # Fill in the dictionary\n",
    "#   for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "#     dc_dict[crs_title] = crs_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import scrapy\n",
    "# import scrapy\n",
    "\n",
    "# # Import the CrawlerProcess\n",
    "# from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# # Create the Spider class\n",
    "# class YourSpider(scrapy.Spider):\n",
    "#   name = 'yourspider'\n",
    "#   # start_requests method\n",
    "#   def start_requests( self ):\n",
    "#     yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "#   def parse(self, response):\n",
    "#     # My version of the parser you wrote in the previous part\n",
    "#     crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "#     crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "#     for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
    "#       dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# # Initialize the dictionary **outside** of the Spider class\n",
    "# dc_dict = dict()\n",
    "\n",
    "# # Run the Spider\n",
    "# process = CrawlerProcess()\n",
    "# process.crawl(YourSpider)\n",
    "# process.start()\n",
    "\n",
    "# # Print a preview of courses\n",
    "# previewCourses(dc_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc575bfddb5c8ca4bb6a4f4dcdd32abc104b5fa4177361381c432fff36ce3e46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
