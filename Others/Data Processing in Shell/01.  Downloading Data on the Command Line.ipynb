{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using curl documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work with command line tools you will often need to consult the documentation to remind yourself of the syntax or of some of the available functionality. In this exercise, you'll consult curl's documentation to answer this question:\n",
    "\n",
    "Based on the information in the curl manual, which of the following is NOT a supported file protocol:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `man curl | grep 'OFTP'`\n",
    "- OFTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading single file using curl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some hands on practice for the more commonly used options and flags with curl. The URL for the hosted file is a shortened URL using tinyurl. Because of that, we need to fill out a flag option that allows for redirected URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Use curl to download the file from the redirected URL\n",
    "curl -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Download and rename the file in the same step\n",
    "curl -o Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading multiple files using curl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 100 data files stored in long sequentially named URLs. Scroll right to see the complete URLs.\n",
    "```\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile001.txt\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile002.txt\n",
    "......\n",
    "https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile100.txt\n",
    "```\n",
    "\n",
    "To minimize having to type the long URLs over and over again, we'd like to download all of these files using a single curl command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Download all 100 data files\n",
    "curl -O https://s3.amazonaws.com/assets.datacamp.com/production/repositories/4180/datasets/files/datafile[001-100].txt\n",
    "\n",
    "# Print all downloaded files to directory\n",
    "ls datafile*.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike curl, there are several ways to download and install wget depending on which operating system your machine is running. Which of the following is NOT a way to install wget?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- On MacOS, install using pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading single file using wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some hands on practice for the option flags that make wget such a popular file downloading tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Fill in the two option flags \n",
    "wget -c -b https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Verify that the Spotify file has been downloaded\n",
    "ls \n",
    "\n",
    "# Preview the log file \n",
    "cat wget-log\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting constraints for multiple file downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following is NOT the correct way to set download constraints for multiple file downloads using wget?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Store all URL locations in a text file (e.g. `url_list.txt`) and iteratively download using `wget` and option flag `i`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating wait time using Wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For download smaller files, enforcing a mandatory wait time between file downloads makes sure we don't overload the server with too many requests. Here, we will using the built in option flag with wget to create a mandatory wait time (in seconds) between downloading each file stored in the URL list file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# View url_list.txt to verify content\n",
    "cat url_list.txt\n",
    "\n",
    "# Create a mandatory 1 second pause between downloading all files in url_list.txt\n",
    "wget --wait=1 -i url_list.txt\n",
    "\n",
    "# Take a look at all files downloaded\n",
    "ls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data downloading with Wget and curl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To kick off a data analysis project, it's good practice to first consolidate all of our data into one place. Often times, this means downloading and pulling data from various locations such as HTTP servers and databases.\n",
    "\n",
    "While curl is handy for downloading a single file, it's somewhat unwieldy for handling multiple file downloads. In this capstone exercise, we will use both curl and Wget to download a series of monthly Spotify files, do some minor processing, and consolidate all downloaded files in our local directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Use curl, download and rename a single file from URL\n",
    "curl -o Spotify201812.zip -L https://assets.datacamp.com/production/repositories/4180/datasets/eb1d6a36fa3039e4e00064797e1a1600d267b135/201812SpotifyData.zip\n",
    "\n",
    "# Unzip, delete, then re-name to Spotify201812.csv\n",
    "unzip Spotify201812.zip && rm Spotify201812.zip\n",
    "mv 201812SpotifyData.csv Spotify201812.csv\n",
    "\n",
    "# View url_list.txt to verify content\n",
    "cat url_list.txt\n",
    "\n",
    "# Use Wget, limit the download rate to 2500 KB/s, download all files in url_list.txt\n",
    "wget --limit-rate=2500k -i url_list.txt\n",
    "\n",
    "# Take a look at all files downloaded\n",
    "ls\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
