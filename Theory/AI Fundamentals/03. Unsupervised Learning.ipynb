{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To which family of dimensionality reduction algorithms does Principal Component Analysis belong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Principal Component Analysis is an example of linear dimensionality reduction algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visitors from outer space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you concluded you must resort to dimensionality reduction because of very limited computational resources you have available for crunching your hyper-dimensional dataset.\n",
    "\n",
    "And for the same reason, you feel that the PCA algorithm is the best choice due to its speed and simplicity.\n",
    "\n",
    "Good. But did you check your data for outliers? Let's see how they could impact your results.\n",
    "\n",
    "A 3-dimensional dataset of 1000 samples (X_raw), slightly \"contaminated\" with 5 outliers (X_new), has been pre-loaded, as seen on Figure 1.\n",
    "\n",
    "On Figure 2 you see that the impact of these outliers (in red) is trivial and creates no problem in extracting actual principal components.\n",
    "\n",
    "But what happens if they are further away?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.031.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add outliers to the blob\n",
    "# X_new, outliers = add_outliers(X_raw,\n",
    "# \t\t\t\t\t\t\t   outlier_distance=200,\n",
    "#                                n_outliers=5)\n",
    "\n",
    "# plot_3d_data(X_new, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.032.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add outliers to the blob\n",
    "# X_new, outliers = add_outliers(X_raw, outlier_distance=200, n_outliers=5)\n",
    "\n",
    "# plot_3d_data(X_new, outliers)  \n",
    "\n",
    "# # Extract principal components\n",
    "# X_2D, outliers_2D = extract_components(X_new, outliers, n_components=2)\n",
    "\n",
    "# # Plot the PCA results\n",
    "# plot_2d_data(X_2D, outliers_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.033.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucky number K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginners in Machine Learning often have very optimistic ideas that Machine Learning can produce amazing insights with little to no human involvement and decision making.\n",
    "\n",
    "The truth is that the performance of your algorithms is heavily influenced by parameters that you as a human define before the model has seen any data.\n",
    "\n",
    "In the case of clustering, most algorithms still require you to be explicit about the number of clusters you are looking for. But not all!\n",
    "\n",
    "Which of the following clustering algorithms determines the number of clusters on its own?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DBSCAN determines the number of clusters on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the right number of clusters is one of the most crucial steps in developing a clustering model.\n",
    "\n",
    "In this exercise, you will apply K-means clustering and the \"elbow method\" to determine the correct number of clusters present in the dataset at hand.\n",
    "\n",
    "The data is loaded in the variable X and you have been provided with two functions for your convenience, plot_clusters() and plot_elbow_curve(), to facilitate the discovery process.\n",
    "\n",
    "Your task is to specify the range of numbers of clusters over which to scan in order to produce the \"elbow curve\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.061.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_range = list(range(2, 6))\n",
    "\n",
    "# summed_distances = []\n",
    "\n",
    "# for k in k_range:    \n",
    "#     kmeans.set_params(n_clusters=k).fit(X)\n",
    "#     summed_distances.append(kmeans.inertia_)\n",
    "\n",
    "# plot_elbow_curve(k_range, summed_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.062.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN is another very popular clustering algorithm, belonging to density-based algorithms.\n",
    "\n",
    "For beginners it can seem very attractive because it doesn't require the number of clusters to be defined in advance.\n",
    "\n",
    "But there's no free lunch and relying on DBSCAN to find the right number of clusters completely on its own can be a big trap.\n",
    "\n",
    "Let's illustrate this by playing with DBSCAN's hyper-parameter eps, which defines the maximum distance between points within the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set eps to 0.1\n",
    "# eps = 0.1\n",
    "\n",
    "# dbscan.set_params(eps=eps)\n",
    "\n",
    "# clusters = dbscan.fit_predict(X)\n",
    "\n",
    "# plot_clusters(X, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.071.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set eps to 0.5\n",
    "# eps = 0.5\n",
    "\n",
    "# dbscan.set_params(eps=eps)\n",
    "\n",
    "# clusters = dbscan.fit_predict(X)\n",
    "\n",
    "# plot_clusters(X, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.072.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set eps to 2\n",
    "# eps = 2\n",
    "\n",
    "# dbscan.set_params(eps=eps)\n",
    "\n",
    "# clusters = dbscan.fit_predict(X)\n",
    "\n",
    "# plot_clusters(X, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.073.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How unsupervised really?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating the performance of Anomaly Detection models, you most often use metrics from the domain of:\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The go-to algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite being a bit more computationally intensive than other methods, an algorithm is commonly used for anomaly detection. Which algorithm is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Isolation Forest is commonly used for anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The odd one out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You saw previously that the IsolationForest() algorithm is a great first choice when in need of anomaly or outlier detection.\n",
    "\n",
    "In this exercise you want to examine how the ratio of inliers to outliers (a.k.a. signal to noise ratio) affects its ability to detect anomalies.\n",
    "\n",
    "The IsolationForest() algorithm has been loaded for you in the variable called isolation_forest, and a helper function make_fake_data() was loaded as well. Your task is to gradually increase the number of outliers and observe the difference in results in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate data comprising of the \"clean\" and \"noisy\" components\n",
    "# noisy_data, true_labels = make_fake_data(n_blobs=2, n_inliers=1000, n_outliers=50)\n",
    "\n",
    "# # Detect anomalies\n",
    "# predicted_anomalies = isolation_forest.fit_predict(noisy_data)\n",
    "    \n",
    "# # Plot results    \n",
    "# plot_detected_anomalies(noisy_data, true_labels, predicted_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.111.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate data comprising of the \"clean\" and \"noisy\" components\n",
    "# noisy_data, true_labels = make_fake_data(n_blobs=2, n_inliers=1000, n_outliers=200)\n",
    "\n",
    "# # Detect anomalies\n",
    "# predicted_anomalies = isolation_forest.fit_predict(noisy_data)\n",
    "    \n",
    "# # Plot results    \n",
    "# plot_detected_anomalies(noisy_data, true_labels, predicted_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.112.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate data comprising of the \"clean\" and \"noisy\" components\n",
    "# noisy_data, true_labels = make_fake_data(n_blobs=2, n_inliers=1000, n_outliers=500)\n",
    "\n",
    "# # Detect anomalies\n",
    "# predicted_anomalies = isolation_forest.fit_predict(noisy_data)\n",
    "    \n",
    "# # Plot results    \n",
    "# plot_detected_anomalies(noisy_data, true_labels, predicted_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.113.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elon's tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will attempt the impossible: detecting patterns in Elon Musk's tweets!\n",
    "\n",
    "You will apply two unsupervised learning algorithms:\n",
    "\n",
    "Dimensionality reduction, to translate your text data into a 2D space.\n",
    "Clustering, to find groups of similar tweets.\n",
    "The go-to model for dimensionality reduction is Principal Component Analysis (PCA), while the KMeans algorithm represents the same in the domain of clustering.\n",
    "\n",
    "Tweets in their raw form were loaded into the variable named tweets_raw.\n",
    "They have also been translated into a machine-digestible, vectorized form, contained in the variable tweets_matrix.\n",
    "To write less code, we want you to use the functions for combined fitting and transformation/prediction - .fit_transform() and .fit_predict()\n",
    "Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the number of dimensions to 2\n",
    "# dimensionality_reducer = PCA(n_components=2)\n",
    "\n",
    "# # Apply dimensionality reduction\n",
    "# tweets_reduced = dimensionality_reducer.fit_transform(tweets_matrix)\n",
    "\n",
    "# # Configure the clustering model\n",
    "# clustering_model = KMeans(n_clusters=2)\n",
    "\n",
    "# # Find clusters\n",
    "# tweet_clusters = clustering_model.fit_predict(tweets_reduced)\n",
    "\n",
    "# # Show the clustering results\n",
    "# print_cluster_tweets(tweet_clusters, tweets_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/03.12.svg\"  style=\"width: 200px, height: 200px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fruits of knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to know why your model has made a certain decision for a specific single record, you are engaging in so-called \"local model interpretation\".\n",
    "\n",
    "Currently, the most popular algorithm for this purpose has a very \"fruity\" acronym. Which one is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting customer churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have just been hired as a Junior Data Scientist for a big telecommunications company.\n",
    "\n",
    "On your first day, you are asked to help with a big problem the company is struggling with: predicting customer churn.\n",
    "\n",
    "Luckily for you, your colleague has already prepared the dataset for you. You just need to use it to train a predictive model and determine its performance.\n",
    "\n",
    "The exercise datasets have been loaded for your convenience:\n",
    "\n",
    "- client_data holds the inputs (gender, tenure, monthly costs, number of dependents, etc)\n",
    "- client_churned holds information on whether this client churned or not ('Yes', 'No')\n",
    "\n",
    "\n",
    "As for the models, you have the RandomForestClassifier and LinearRegression at your disposal -- choose wisely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your model\n",
    "# model = RandomForestClassifier()\n",
    "\n",
    "# # Divide the data into the training and testing set and train the model\n",
    "# X_train, X_test, y_train, y_test = train_test_split(client_data, client_churned)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Generate predictions on the test set\n",
    "# predictions_test = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model predictions using metrics appropriate for this problem class\n",
    "# print_metrics(target_test=y_test,\n",
    "#               predictions=predictions_test, \n",
    "#               metrics=['accuracy', 'precision', 'recall'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('env_py')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e949e87132dd83f1a7623eb88007e3532b03b66b77111be347aa4a383049722"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
